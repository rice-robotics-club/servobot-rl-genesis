algorithm:
  class_name: PPO
  clip_param: 0.2
  desired_kl: 0.01
  entropy_coef: 0.03
  gamma: 0.99
  lam: 0.95
  learning_rate: 0.001
  max_grad_norm: 1.0
  num_learning_epochs: 5
  num_mini_batches: 4
  schedule: adaptive
  use_clipped_value_loss: true
  value_loss_coef: 1.0
init_member_classes: {}
policy:
  activation: elu
  actor_hidden_dims:
    - 512
    - 256
    - 128
  critic_hidden_dims:
    - 512
    - 256
    - 128
  init_noise_std: 1.0
  class_name: ActorCritic
runner:
  checkpoint: -1
  experiment_name: servobot
  load_run: -1
  log_interval: 1
  max_iterations: 1000
  record_interval: -1
  resume: false
  resume_path: null
  run_name: ''
runner_class_name: OnPolicyRunner
num_steps_per_env: 24
save_interval: 100
empirical_normalization: null
seed: 1